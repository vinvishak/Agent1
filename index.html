<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Simple Voice Transcriber</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    body {
      margin: 0;
      padding: 24px;
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
      background: #0f172a;
      color: #e5e7eb;
    }

    h1 {
      margin-top: 0;
      margin-bottom: 8px;
      font-size: 1.5rem;
    }

    p {
      margin-top: 0;
      margin-bottom: 16px;
      font-size: 0.95rem;
      color: #94a3b8;
    }

    .buttons {
      margin-bottom: 16px;
    }

    button {
      padding: 8px 16px;
      margin-right: 8px;
      border-radius: 6px;
      border: none;
      font-size: 0.95rem;
      font-weight: 600;
      cursor: pointer;
    }

    #start-btn {
      background: #22c55e;
      color: #052e16;
    }

    #stop-btn {
      background: #ef4444;
      color: #fef2f2;
    }

    #status {
      font-size: 0.9rem;
      margin-bottom: 12px;
      color: #facc15;
    }

    #transcript {
      width: 100%;
      min-height: 160px;
      padding: 10px;
      border-radius: 8px;
      border: 1px solid #1e293b;
      background: #020617;
      color: #e5e7eb;
      font-size: 0.95rem;
      line-height: 1.5;
      white-space: pre-wrap;
    }

    .note {
      margin-top: 12px;
      font-size: 0.8rem;
      color: #64748b;
    }
  </style>
</head>
<body>
  <h1>Simple Voice Transcriber</h1>
  <p>Click Start, allow microphone access, speak, then click Stop. The audio is sent to your backend for transcription.</p>

  <div class="buttons">
    <button id="start-btn">Start</button>
    <button id="stop-btn">Stop</button>
  </div>

  <div id="status">Status: idle</div>

  <div id="transcript">Transcript will appear here...</div>

  <p class="note">
    This records audio in your browser and sends it to your FastAPI + Whisper backend on <code>http://127.0.0.1:8000</code>.
  </p>

  <script>
    // URL of your backend API
    // Make sure your FastAPI server is running here:
    // uvicorn main:app --reload --host 127.0.0.1 --port 8000
    const API_BASE = "http://127.0.0.1:8000";

    // Get references to DOM elements
    const startBtn = document.getElementById("start-btn");
    const stopBtn = document.getElementById("stop-btn");
    const statusEl = document.getElementById("status");
    const transcriptEl = document.getElementById("transcript");

    // These will be used for recording
    let mediaRecorder = null;  // MediaRecorder instance
    let chunks = [];           // Array of recorded audio data chunks

    function setStatus(message) {
      statusEl.textContent = "Status: " + message;
    }

    // Start recording when Start is clicked
    startBtn.onclick = async () => {
      // If already recording, do nothing
      if (mediaRecorder && mediaRecorder.state === "recording") {
        return;
      }

      try {
        // Ask the user for microphone permission and get an audio stream
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });

        // Create a MediaRecorder for this stream
        mediaRecorder = new MediaRecorder(stream);
        chunks = [];  // reset chunks

        // Each time there is audio data available, store it in the chunks array
        mediaRecorder.ondataavailable = event => {
          if (event.data.size > 0) {
            chunks.push(event.data);
          }
        };

        // When recording stops, build the audio Blob and send it to the backend
        mediaRecorder.onstop = async () => {
          // Combine all recorded chunks into one Blob
          const blob = new Blob(chunks, { type: "audio/webm" });

          // Prepare form data to send to the backend
          const formData = new FormData();
          formData.append("audio", blob, "recording.webm");
          // For now, use a fixed speaker label
          formData.append("speaker", "Speaker 1");

          setStatus("Uploading and transcribing...");

          try {
            // Send POST request to /upload endpoint on the backend
            const res = await fetch(API_BASE + "/upload", {
              method: "POST",
              body: formData,
            });

            if (!res.ok) {
              throw new Error("Upload failed with status " + res.status);
            }

            // Parse the JSON response: { id, speaker, transcript, created_at }
            const data = await res.json();

            // Show transcript in the transcript div
            transcriptEl.textContent = data.transcript || "No transcript returned.";

            // Update status
            const time = new Date(data.created_at).toLocaleString();
            setStatus("Transcription complete at " + time);
          } catch (err) {
            console.error("Error uploading or transcribing:", err);
            setStatus("Error uploading or transcribing");
          }
        };

        // Start recording
        mediaRecorder.start();
        setStatus("Recording... speak now");
        transcriptEl.textContent = "Recording... transcript will appear here after you stop.";
      } catch (err) {
        console.error("Error starting recording:", err);
        setStatus("Error accessing microphone");
      }
    };

    // Stop recording when Stop is clicked
    stopBtn.onclick = () => {
      if (mediaRecorder && mediaRecorder.state === "recording") {
        mediaRecorder.stop();
        setStatus("Stopping and processing...");
      }
    };
  </script>
</body>
</html>
